# 9장 웹 크롤러 설계

### 웹 크롤러의 종류

- 검색 엔진 인덱싱
- 웹 아카이빙
- 웹 마이닝
- 웹 모니터링

## 문제 인식

#### 기본 알고리즘

1. URL 집합이 주어지면 URL들이 가리키는 모든 웹 페이지를 다운로드
2. URL을 추출
3. 추출된 URL을 목록에 추가하고 1번으로 다시 돌아감

##### 주어진 조건 이외에 필수로 만족시켜야하는 요소

- 규모 확장성
- 안정성
- 예절
- 확장성

##### 개략적인 규모 추정

- 매년 10억 개의 웹페이지 다운
- QPS 최대 800
- 30PB 저장 용량

#### 크롤링 요소

- 미수집 URL 저장소
    - 다운로드할 URL
    - 다운로드 된 저장소
    - 큐로 구성
- 도메인 이름 변환기
- 콘텐츠 파서
- 콘텐츠 저장소
- 중복 콘텐츠 체크
- URL 추출기
- URL 필터
    - 확장자 있는 URL (배제 대상)
- 방문 URL 정리
    - URL 저장소

#### Workflow

1. 시작URL을 미수집 URL 저장소에 저장
2. HTML 다운로더는 미수집 URl 저장소에서 목록 가져옴
3. 도메인 이름 변환기 사용하여 URL IP주소를 알아내고, 웹페이지 다운
4. 페이지 검증
5. 중복 콘텐츠 확인 절차
6. 신규 URL 추출
7. 중복 검증

### 설계 방법

#### DFS vs BFS

- 일반적으로 BFS
    - 깊이 측정이 어려움
    - 다만 같은 서버로 돌아가는 문제점
    - 이를 위해 미수집 URL 저장소 이용
    - RANK를 두어 접속 우선순위 둠

#### 예의

- 태스크 간 시간차를 두고 실행
- 후면큐

#### 랭크

- 순위 우선 결정 장치를 사용하여 설계
- 전면 큐를 사용해 결정

#### 신선도

- 변경 이력 확인
- 우선 순위를 통해 갱신 순위 조절

#### HTML 다운로더

##### Robot.txt

- 웹 페이지 상의 규칙을 확인 가능
- 주기적으로 캐시에 보관

#### 성능 최적화

1. 분산 크롤링
2. DNS 파서
    - Cron을 통해 주기적 갱신
3. 지역성
4. 짧은 타임 아웃

#### 안정성

- 안정 해시 : 부하 분산을 적용하는 기술
- 크롤링 상태 / 수집 데이터 저장 : 복구 기술
- 예외 처리 : 예외가 발생해도 작업을 다시 이어갈 수 있게
- 데이터 검증 : 시스템 오류 방지

#### 확장성

- 새로운 형태의 콘텐츠를 지원할 수 있도록 설계

#### 문제 있는 콘텐츠 감지

- 중복 콘텐츠 탐지
- 거미 덫
    - URL 길이로 탐지
    - 수작업
- 데이터 노이즈

#### 마무리

- 서버 측 렌더링
    - 그 사이트에서 즉석으로 만들어내는 링크 탐지
- 페이지 필터링
- 다중화, 샤딩
- 수평적 확장성
- 가용성,일관성, 안정성
- 데이터 분석 솔루션

